# Problem Statement :-

### Given a customer-level data of a leading telecom firm which contains information of customers with some attributes. Design a predictive model to identify the customers which are at high risk of churn i.e. customers which are more likely to cancel subscription of the telecom company.


# Objective :-

### In the telecom industry, customers are able to choose from various service providers and can switch from one operator to other. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate, therefore, it is very important for the companies to retain their customers. The cost of retaining an existing customer is far less than acquiring a new one. This model will help the companies to know the factors which are significant in customer churn so they can work on them. This model will help the company to retain their existing customers.


# Reading the dataset
```{r}
telecom_churn = read.csv("telecom_churn.csv")
```


```{r}
View(telecom_churn)           # to view data
```


```{r}
dim(telecom_churn)            # dimension of data
```


```{r}
str(telecom_churn)            # structure of data
```
```{r}
original_data = telecom_churn         # copying the data
```

```{r}
a = sample(nrow(telecom_churn), 1000)   # taking random sample of 1000 observations
a[1:20]
```
# Train-Test Split

### We split the data into train and test. We will perform EDA only on Train data keeping the test data aside.

```{r}
train_data = telecom_churn[-a, ]       # train data except 1000
dim(train_data)
```

```{r}
test_data = telecom_churn[a, ]        # sample of 1000 in test which are not in train
dim(test_data)
```

# Check missing values

### We are checking whether the dataset contains any missing value. If missing value is present, we should take appropriate action like imputing the missing value or dropping the column if the percentage of it is high.
### In R we can check the missing value with the help of the complete.cases() function. It gives us the number of observations which don't have any missing value present in it. We can then subtract it with the total number of observations to get the number of observations which have missing values in it.

```{r}
sum(complete.cases(telecom_churn))    # gives complete cases i.e. which are not having missing values
```
```{r}
nrow(telecom_churn) - sum(complete.cases(telecom_churn))   # no. of observations with missing values
```

### We found out that there are no missing values in the dataset. We can now proceed with the further analysis.


# Univariate analysis

### In univariate analysis, we will look at each variable separately and analyse it. We will first identify that which are the numerical and which are the categorical variables in our dataset. After that we will see the distribution of each variable. For that we will plot the distribtion of each variable based on its type. If it is a numerical variable, we will plot histogram and boxplot for it. If it is categorical in nature we will plot barplot for it. In the distributions of numerical variable, we will see whether the distribution is skewed or the variable is having outliers.

```{r}
str(telecom_churn)
```

### After looking at the structure of data, we can see that variables (State, International.plan, Voice.mail,plan and Churn) are categorical variables and rest other are numerical variables. However, the numerical variables can also be categorical. For example, the variable area code, it is used to categorise the area to which the customer belongs. Checking whether it is categorical or not.

```{r}
unique(train_data$Area.code)     # unique values in area code
```
### Only 3 unique values are there it means the user belongs to any of these three areas only. It is a categorical variable.

# Converting categorical variable into factors :-

```{r}
# Converting the categorical variables in factors
train_data$State = as.factor(train_data$State)
train_data$International.plan = as.factor(train_data$International.plan)
train_data$Voice.mail.plan = as.factor(train_data$Voice.mail.plan)
train_data$Churn = as.factor(train_data$Churn)
train_data$Area.code = as.factor(train_data$Area.code)
```

# Plotting the Graphs :-

```{r}
for (i in 1:ncol(train_data)) 
{
  if(is.numeric(train_data[, i]))     # Histogram and boxplot if numeric
  {
    par(mfrow = c(2,1))
    boxplot(train_data[, i], xlab = names(train_data)[i], horizontal = T, main = paste("Boxplot of", names(train_data)[i]))
    hist(train_data[, i], xlab = names(train_data)[i], main = paste("Histogram of", names(train_data)[i]))
  }
  
  else if (is.factor(train_data[, i]))      # barplot if categorical
  {
    par(mfrow = c(1,1))
    barplot(table(train_data[, i]), xlab = names(train_data)[i], main = paste("Barplot of", names(train_data)[i]))
  }
}
```


# Observations from Univariate analysis :-

### 1. The target variable is highly imbalanced. Model will find it difficult to learn the minority class observations as it is very less in numbers as compare to the majority class. Therefore, we will have to balance the class of target variable to get better result.

### 2. Area code 415 has highest number of customers. We have to see that whether customers of this area are facing problems and churning. So we can improve the services here.

### 3. Only few customers are having international plan, we will have to see whether they are facing problems and churning. Whether company is not good in providing international call facilities and that is why customers are churning. Similarly for voice mail plan.

### 4. The distribution of total calls, charge and minutes for day, evening and night is almost normal or slightly skewed. For these variables, we should analyse whether the people who are making more calls, talking for more minutes or are charged more, are these customers facing problems. Or the people making less calls are facing any problem with the network.

### 5. Similarly the customers who are making international calls are facing any issues with the network or the quality of service provided to them.

### 6. Customer Service Calls :- This seems like an important variable because customer usually make these calls when they are unsatisfied with the service provided to them. We should check whether the people who are making more customer service calls are churning more.


# Anomaly Detection :-

### In anomaly detection, we should check whether the number of minutes for a customer is more than 1440 minutes that is greater than 1 day. It should not be greater than 1 day because the data is having average minutes for 1 day. Similarly we can look for the number of calls and total charge as well to see if they are very high.

```{r}
max(train_data$Total.day.minutes + train_data$Total.eve.minutes + train_data$Total.night.minutes)
```
```{r}
max(train_data$Total.day.calls + train_data$Total.eve.calls + train_data$Total.night.calls)
```
```{r}
max(train_data$Total.day.charge + train_data$Total.eve.charge + train_data$Total.night.charge)
```

### We found out that there is no anomaly in the data.

# Outlier Treatment :-

### Looking at the plots in univariate analysis, we can see that there are few outliers in each numerical variable. We are deciding not to remove that because they are important in the analysis of the problem.

### For example :- 

```{r}
par(mfrow = c(2,1))
boxplot(train_data[, "Customer.service.calls"], xlab = "Customer.service.calls", horizontal = T, main = "Boxplot of Customer.service.calls")
hist(train_data[, 'Customer.service.calls'], xlab = "Customer.service.calls", main = "Histogram of Customer.service.calls")
```

### If we take the example of customer service calls, we cannot simply remove the outliers which is more than 2.5 calls in this case. Because there can be a reason why the customers are doing more calls to customer service. Maybe they are unhappy with the services and therefore are at higher risk of churn.

### Similarly for all other numerical variables, there are outliers but they can be the deciding factors on whether the customer churn or not. Therefore we have decided to let them as it is, because they carry significant information.


# Bi-variate Analysis :-

### In bi-variate analysis, we will check how each of the predictor is related with the target variable. We are doing this by plotting the side by side box plot if the predictor is numerical and making a side by side bar plot if the predictor is categorical.

### By doing this we are checking if we can find any difference between the customers who are churning and those who are not with respect to each predictor. This will help us to analyse which are important predictors which can distinguish the churned customers from those who did not.

### We can also see the correlation of the continuous numerical variables. If we find variables which are highly correlated, then we can take further action on them or can consider to drop 1 of them. This step will be done at the time of fitting the model.

```{r}
for (i in 1:ncol(train_data)-1) 
{
  if(is.numeric(train_data[, i]))       # side by side boxplot for numerical
  {
    par(mfrow = c(1,1))
    boxplot(train_data[, i] ~ train_data$Churn, xlab = "Churn", ylab = names(train_data)[i], horizontal = F, notch=T, main = paste("Boxplot of", names(train_data)[i]))
  }
  
  else if (is.factor(train_data[, i]))     # side by side bar plot for categorical
  {
    par(mfrow = c(1,1))
    barplot(table(train_data$Churn, train_data[, i]), col = c("green", "red"), beside = TRUE, legend.text = c("Not churn", "Churn"), xlab = names(train_data)[i], main = paste("Barplot of", names(train_data)[i]))
  }
}
```

```{r}
tapply(train_data$Total.day.charge, train_data$Churn, FUN=mean)  # mean of total day charge of user churning and not
```


```{r}
tapply(train_data$Customer.service.calls, train_data$Churn, FUN=median)   # median of total day charge of user churning and not
```


# Observations from Bi-variate Analysis :-

### 1. We found out that in some states the ratio of churn to not churn is high as compare to others. This shows that the variable state is significant and we can specifically focus on the states which are having this problem.

### 2. Those customers who are having international plans are churning more.

### 3. Customers who are churning have high number of voice mail messages. If we had removed the outliers, we wouldn't have find this issue.

### 4. Customers who are churning have higher total day charge and total day minutes.

### 5. Customers who are churning are making more customer service calls. If we had removed the outliers, we wouldn't have find this issue.


# T-test :-

### We will do the T-test on the numerical predictors and the target variable. This will tell us whether the numerical predictor is significant or not. The predictor is significant means that the mean of the predictor of people who churn is significantly different from mean of the predictor of the people who do not churn. 

```{r}
TTest <- function(data, cols = names(data), target)
{
  # checking whether the data given is a dataframe or not
  if(!is.data.frame(data))
    stop("The given object is not a dataframe")
  
  # checking whether the target variable is binary or not
  if(length(unique(data[, target])) != 2)
    stop("The given target variable is not binary")
  
  pvalues = c()                      # created an empty vector to store the p-values
  
  for (i in cols)                    # loop will go through all the columns of data
  {
    if(is.numeric(data[, i]))        # check whether the column is numeric or not
    {
      if(i != target)                # Since we dont have to do t-test with the same column
      {
        # perform t-test on the column in loop and the target column
        a = t.test(data[, i] ~ data[, target])
        pvalues[i] = a$p.value       # stores the p-value of t-test into the vector temp
      }
    }  
  }
  # converting pvalues into dataframe
  pvalues = as.data.frame(pvalues)
  return(pvalues)
}
```

```{r}
#TTest()
TTest(train_data, target = "Churn")
```

# Observation after T-test :-

### After performing the T-test, we found out that for 95% confidence interval, variables (Number.vmail.messages, Total.day.minutes, Total.day.charge, Total.eve.minutes, Total.eve.charge) are significant as they have pvalue < 0.05.

# Chi-square Test :-

### We will do the T-test on the categorical predictors and the target variable. This will tell us whether the categorical predictor is significant or not. The chi square test tests the independence between the categorical variables. The predictor is significant means that there is dependence of the target variable on the predictor. 

```{r}
ChiTest <- function(data, cols = names(data), target)
{
  # checking whether the data given is a dataframe or not
  if(!is.data.frame(data))
    stop("The given object is not a dataframe")
  
  # checking whether the target variable is binary or not
  if(length(unique(data[, target])) != 2)
    stop("The given target variable is not binary")
  
  pvalues = c()                      # created an empty vector to store the p-values
  
  for (i in cols)                    # loop will go through all the columns of data
  {
    if(is.factor(data[, i]))         # check whether the column is factor or not
    {
      if(i != target)                # Since we dont have to do chisq test with the same column
      {
        # perform chisq on the column in loop and the target column
        c = chisq.test(table(data[, i], data[, target]))
        pvalues[i] = c$p.value       # stores the p-value of chisq test into the vector temp
      }
    }  
  }
  # converting into dataframe
  pvalues = as.data.frame(pvalues)   
  return(pvalues)
}
```


```{r}
ChiTest(train_data, target = "Churn")
```

# Observations after the Chi-square test :-

### After performing the chi square test, we found out that for 95% confidence interval, variables (state, International plan and voice.mail,plan) are significant as they have pvalue < 0.05. It means our target variable is dependent on these predictors.